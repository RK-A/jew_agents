from langgraph.graph import StateGraph, START, END
from .utils.state import TrendState
from .utils.nodes import (
    extraction_node, 
    calculation_node, 
)
import logging
from typing import Dict, Any, List

import json
from agents.base_agent import BaseAgent

logger = logging.getLogger(__name__)




class TrendAgent(BaseAgent):
    """Agent for analyzing fashion trends from journals and articles using LangGraph"""
    
    DEFAULT_SYSTEM_PROMPT = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –º–æ–¥—ã. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON."""
    
    
    def __init__(self, llm_provider, rag_service=None, language: str = "auto", custom_system_prompt: str = None):
        super().__init__(llm_provider, rag_service, language, custom_system_prompt)
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """Build LangGraph workflow for trend analysis process"""
        workflow = StateGraph(TrendState)

        # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
        workflow.add_node("extract", extraction_node)
        workflow.add_node("analyze", self._analysis_node)
        workflow.add_node("calculate", calculation_node)
        workflow.add_node("report", self._reporting_node)

        # –°–≤—è–∑–∏ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ)
        workflow.add_edge(START, "extract")
        workflow.add_edge("extract", "analyze")
        workflow.add_edge("analyze", "calculate")
        workflow.add_edge("calculate", "report")
        workflow.add_edge("report", END)
        
        return workflow.compile()
    
    async def process(self, content: str) -> Dict[str, Any]:
        """
        Process trend analysis from fashion journal content using LangGraph workflow
        
        Args:
            content: Fashion journal article or content text
        
        Returns:
            Dict with trends, keywords, and recommendations
        """
        try:
            self.logger.info(f"Starting trend analysis on {len(content)} characters of content...")
            
            # Initialize state
            initial_state: TrendState = {
                "content": content,
                "extracted_keywords": {},
                "trends": {},
                "trend_scores": {},
                "emerging_trends": [],
                "recommendations": [],
                "report": ""
            }
            
            # Run graph
            final_state = await self.graph.ainvoke(initial_state)
            
            # Return result
            if final_state.get("error"):
                return {
                    "status": "error",
                    "message": final_state["error"]
                }
            
            return {
                "status": "success",
                "extracted_keywords": final_state["extracted_keywords"],
                "trends": final_state["trends"],
                "trend_scores": final_state["trend_scores"],
                "emerging_trends": final_state["emerging_trends"],
                "recommendations": final_state["recommendations"],
                "report": final_state["report"],
                "content_length": len(content)
            }
        
        except Exception as e:
            self.logger.error(f"Error in trend analysis: {e}", exc_info=True)
            return {
                "status": "error",
                "message": str(e)
            }
    
    async def _analysis_node(self, state: TrendState):
        print("--- [2/4] Analyzing Trends (LLM) ---")
        content = state["content"]
        keywords = state["extracted_keywords"]
        
        # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ü–û–õ–ù–´–ô –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ trend_agent.py
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –æ –º–æ–¥–µ –∏ –∏–∑–≤–ª–µ–∫–∏ —é–≤–µ–ª–∏—Ä–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã.
        
        –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:
        {json.dumps(keywords, indent=2, ensure_ascii=False)}
        
        –¢–µ–∫—Å—Ç:
        {content[:2000]}...
        
        –û–ø—Ä–µ–¥–µ–ª–∏:
        1. –¢–æ–ø-5 –º–æ–¥–Ω—ã—Ö —Å—Ç–∏–ª–µ–π
        2. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
        3. –¢—Ä–µ–Ω–¥–æ–≤—ã–µ —Ü–≤–µ—Ç–∞
        4. –£–ø–æ–º—è–Ω—É—Ç—ã–µ –¥–∏–∑–∞–π–Ω–µ—Ä—ã/–±—Ä–µ–Ω–¥—ã
        5. –°–µ–∑–æ–Ω–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑
        
        –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
        {{
            "trending_styles": ["—Å—Ç–∏–ª—å1", ...],
            "popular_materials": ["–º–∞—Ç–µ—Ä–∏–∞–ª1", ...],
            "trending_colors": ["—Ü–≤–µ—Ç1", ...],
            "mentioned_designers": ["–∏–º—è", ...],
            "seasonal_forecast": "–æ–ø–∏—Å–∞–Ω–∏–µ —Å–µ–∑–æ–Ω–∞"
        }}

        JSON:"""
        

        response = await self.llm.generate(
            prompt=prompt,
            context=self.DEFAULT_SYSTEM_PROMPT
        )
        # print(response)
        # 2. –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ JSON (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ + –∑–∞—â–∏—Ç–∞ –æ—Ç markdown)
        txt = response
        # –£–¥–∞–ª—è–µ–º ```json –∏ ```
        if txt.startswith("```json"):
            txt = txt.split("```json").split("```").strip()[1]
        elif txt.startswith("```"):
            txt = txt.split("```")[6].split("```")[0].strip()
            
        try:
            trends = json.loads(txt)
        except Exception as e:
            print(f"JSON Parsing Error: {e}. Using fallback logic.")
            # Fallback –ª–æ–≥–∏–∫–∞ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞: –±–µ—Ä–µ–º —Ç–æ–ø—ã –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            trends = {
                "trending_styles": [kw["keyword"] for kw in keywords.get("styles", [])[:5]],
                "popular_materials": [kw["keyword"] for kw in keywords.get("materials", [])[:5]],
                "trending_colors": [kw["keyword"] for kw in keywords.get("colors", [])[:3]],
                "mentioned_designers": [],
                "seasonal_forecast": "Unable to determine from LLM error"
            }
            
        return {"trends": trends}

    async def _reporting_node(self, state: TrendState):
        print("--- [4/4] Writing Report ---")
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ _generate_trend_report
        context = f"""
        === –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ===
        {json.dumps(state['extracted_keywords'], indent=2, ensure_ascii=False)}
        
        === –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã ===
        {json.dumps(state['trends'], indent=2, ensure_ascii=False)}
        
        === –ù–æ–≤—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è ===
        {json.dumps(state['emerging_trends'], indent=2, ensure_ascii=False)}
        
        === –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ===
        {json.dumps(state['recommendations'], indent=2, ensure_ascii=False)}
        """
        
        prompt = f"""–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –º–æ–¥—ã, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —é–≤–µ–ª–∏—Ä–Ω—ã—Ö –∏–∑–¥–µ–ª–∏—è—Ö –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä–∞—Ö.
        –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ —Ç—Ä–µ–Ω–¥–∞—Ö:
        {context}
        
        –û—Ç—Ä—ã–≤–æ–∫ –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏:
        {state['content'][:1000]}...
        
        –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–ß–ï–¢–£ (Markdown):
        
        # 1. –†–µ–∑—é–º–µ
        –ö—Ä–∞—Ç–∫–æ (3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è): –≥–ª–∞–≤–Ω–∞—è –∏–¥–µ—è —Å–µ–∑–æ–Ω–∞, –∫–ª—é—á–µ–≤–æ–π –¥—Ä–∞–π–≤–µ—Ä –ø—Ä–æ–¥–∞–∂.
        
        # 2. –ê–Ω–∞–ª–∏–∑ –¢—Ä–µ–Ω–¥–æ–≤ (–¢–∞–±–ª–∏—Ü–∞)
        –°–æ–∑–¥–∞–π Markdown-—Ç–∞–±–ª–∏—Ü—É —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "–ö–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã", "–°—Ç–∞—Ç—É—Å" (–†–∞—Å—Ç—É—â–∏–π/–ü–∏–∫–æ–≤—ã–π/–£–≥–∞—Å–∞—é—â–∏–π).
        –û–ø–∏—à–∏ –°—Ç–∏–ª–∏, –ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –¶–≤–µ—Ç–∞.
        
        # 3. –¢–æ–≤–∞—Ä–Ω–∞—è –ú–∞—Ç—Ä–∏—Ü–∞ (–ö–∞—Ç–µ–≥–æ—Ä–∏–∏)
        –ö–∞–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —É–∫—Ä–∞—à–µ–Ω–∏–π (–ö–æ–ª—å—Ü–∞, –°–µ—Ä—å–≥–∏...) —Å–µ–π—á–∞—Å –ª–∏–¥–∏—Ä—É—é—Ç –ø–æ —É–ø–æ–º–∏–Ω–∞–µ–º–æ—Å—Ç–∏? –î–∞–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Ü–∏—Ñ—Ä–∞–º –∏–∑ Share of Voice.
        
        # 4. Action Plan (–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)
        –ü—Ä–µ–≤—Ä–∞—Ç–∏ JSON-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç. –†–∞–∑–¥–µ–ª–∏ –Ω–∞:
        * üì¶ –ó–∞–∫—É–ø–∫–∏ (–ß—Ç–æ –ø–æ–∫—É–ø–∞—Ç—å –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å)
        * üé® –î–∏–∑–∞–π–Ω (–ß—Ç–æ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å)
        * üì¢ –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ (–û —á–µ–º –≥–æ–≤–æ—Ä–∏—Ç—å —Å –∫–ª–∏–µ–Ω—Ç–æ–º)
        
        # 5. –ò–Ω—Å–∞–π—Ç—ã
        –ï—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –∏–º–µ–Ω–∞ –¥–∏–∑–∞–π–Ω–µ—Ä–æ–≤ –∏–ª–∏ —Å–µ–∑–æ–Ω–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ø–æ–º—è–Ω–∏ –∏—Ö –∫–∞–∫ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏.
        
        –°—Ç–∏–ª—å –ø–∏—Å—å–º–∞: –î–µ–ª–æ–≤–æ–π, –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã. –ò—Å–ø–æ–ª—å–∑—É–π –∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–æ–≤.
        """
        
        response = await self.llm.generate(
            prompt=prompt
            )
        return {"report": response}