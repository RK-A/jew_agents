# Whisper Audio Transcription - Context

## –ß—Ç–æ —ç—Ç–æ

API endpoint –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **–ª–æ–∫–∞–ª—å–Ω—ã–π Whisper** (–±–µ—Å–ø–ª–∞—Ç–Ω–æ) –∏–ª–∏ **OpenAI API** (–ø–ª–∞—Ç–Ω–æ).

## üì° Endpoint

```
POST /api/transcribe
Content-Type: multipart/form-data
Field: audio (webm, mp3, wav, m4a, ogg, flac)

Response: { "text": "...", "language": "ru" }
```

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (.env)

### –õ–æ–∫–∞–ª—å–Ω—ã–π Whisper (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
```env
WHISPER_USE_LOCAL=true
WHISPER_MODEL=base              # tiny, base, small, medium, large-v2, large-v3
WHISPER_LANGUAGE=ru
# –ù–µ –Ω—É–∂–Ω—ã API –∫–ª—é—á–∏!
```

**–ú–æ–¥–µ–ª–∏:**
- `base` (74MB) - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å, **—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**
- `small` (244MB) - –ª—É—á—à–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
- `tiny` (39MB) - —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è, –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
- `medium/large` - –º–µ–¥–ª–µ–Ω–Ω–æ, –æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ

### OpenAI API (–ø–ª–∞—Ç–Ω–æ: $0.006/–º–∏–Ω)
```env
WHISPER_USE_LOCAL=false
WHISPER_API_KEY=sk-...
WHISPER_MODEL=whisper-1
WHISPER_LANGUAGE=ru
```

## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### Frontend (TypeScript)
```typescript
export const transcribeAudio = async (audioBlob: Blob): Promise<string> => {
  const formData = new FormData();
  formData.append('audio', audioBlob, 'recording.webm');

  const response = await fetch(`${API_URL}/transcribe`, {
    method: 'POST',
    body: formData,
  });

  const data = await response.json() as { text: string };
  return data.text;
};

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å MediaRecorder
const mediaRecorder = new MediaRecorder(stream);
const chunks: Blob[] = [];
mediaRecorder.ondataavailable = (e) => chunks.push(e.data);
mediaRecorder.onstop = async () => {
  const audioBlob = new Blob(chunks, { type: 'audio/webm' });
  const text = await transcribeAudio(audioBlob);
  await sendMessage(text);  // –û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ —á–∞—Ç
};
mediaRecorder.start();
```

### Python
```python
import httpx

async def transcribe(audio_path: str) -> str:
    async with httpx.AsyncClient() as client:
        with open(audio_path, 'rb') as f:
            response = await client.post(
                'http://localhost:8000/api/transcribe',
                files={'audio': f}
            )
        return response.json()['text']
```

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

–£–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ `requirements.txt`:
```
faster-whisper>=0.10.0
```

Docker –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω —Å –Ω–æ–≤–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å—é.

## üîß Backend —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

**–§–∞–π–ª—ã:**
- `audio/whisper_service.py` - OpenAI API (–µ—Å–ª–∏ WHISPER_USE_LOCAL=false)
- `audio/local_whisper_service.py` - –õ–æ–∫–∞–ª—å–Ω—ã–π Whisper (–µ—Å–ª–∏ WHISPER_USE_LOCAL=true)
- `backend/routes.py` - Endpoint `/api/transcribe`
- `backend/dependencies.py` - `get_whisper_service()` dependency
- `config.py` - –ù–∞—Å—Ç—Ä–æ–π–∫–∏

**–õ–æ–≥–∏–∫–∞:**
1. Frontend –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª –Ω–∞ `/api/transcribe`
2. Backend –≤—ã–±–∏—Ä–∞–µ—Ç —Å–µ—Ä–≤–∏—Å (–ª–æ–∫–∞–ª—å–Ω—ã–π –∏–ª–∏ API) –ø–æ –∫–æ–Ω—Ñ–∏–≥—É
3. –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
4. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è JSON —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

**–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞**: `WHISPER_MODEL=base`  
**–ü—Ä–æ–¥–∞–∫—à–µ–Ω (CPU)**: `WHISPER_MODEL=small`  
**–ü—Ä–æ–¥–∞–∫—à–µ–Ω (GPU)**: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å CUDA –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `large-v3`

## üêõ Troubleshooting

**–û—à–∏–±–∫–∞ "faster-whisper not installed"** ‚Üí Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å—é, –¥–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å  
**–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è** ‚Üí –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ GPU  
**–ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ** ‚Üí –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å: `small` –∏–ª–∏ `medium`
